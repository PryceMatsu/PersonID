{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCOLkh6yhzWn",
        "outputId": "dce7644c-8621-4faa-9ca1-e9055ebfefc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuLObSQ1YHTA",
        "outputId": "8cf05830-66bb-48d5-f49e-e90ccb32c1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded class names: ['person_ben', 'person_kenny', 'person_pryce']\n",
            "Number of classes: 3\n",
            "Trained classification model loaded successfully from /content/drive/MyDrive/person_classifier_model.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import os\n",
        "import json # loads the class names\n",
        "\n",
        "MODEL_SAVE_PATH_CLASSIFIER = '/content/drive/MyDrive/person_classifier_model.pth'\n",
        "CLASSES_SAVE_PATH = '/content/drive/MyDrive/person_reid_classes.json'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load class names to know num_classes\n",
        "# To load our saved model weights, PyTorch needs to know the exact \"shape\" or architecture\n",
        "# of the model they belong to. We start by loading the class names to find out how many\n",
        "# output neurons our final layer should have (one for each person).\n",
        "try:\n",
        "    with open(CLASSES_SAVE_PATH, 'r') as f:\n",
        "        class_names = json.load(f)\n",
        "    num_classes = len(class_names)\n",
        "    print(f\"Loaded class names: {class_names}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"{CLASSES_SAVE_PATH} not found.\")\n",
        "\n",
        "# Now, we build a \"shell\" of the ResNet18 model. It has the same layers as our trained model,\n",
        "# but with random, uninitialized weights for now.\n",
        "model_to_load = models.resnet18(weights=None) # Start with an uninitialized ResNet18\n",
        "num_ftrs = model_to_load.fc.in_features\n",
        "\n",
        "# We replace the final fully connected (fc) layer with one that matches our specific task\n",
        "# ex. outputting a score for each of our 3 people\n",
        "model_to_load.fc = nn.Linear(num_ftrs, num_classes)\n",
        "model_to_load = model_to_load.to(device)\n",
        "\n",
        "# Load the saved weights\n",
        "if os.path.exists(MODEL_SAVE_PATH_CLASSIFIER):\n",
        "    model_to_load.load_state_dict(torch.load(MODEL_SAVE_PATH_CLASSIFIER, map_location=device))\n",
        "    model_to_load.eval() # Set the model to evaluation mode\n",
        "    print(f\"Trained classification model loaded successfully from {MODEL_SAVE_PATH_CLASSIFIER}\")\n",
        "else:\n",
        "    print(f\"Model file not found at {MODEL_SAVE_PATH_CLASSIFIER}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYfEW0snYWke",
        "outputId": "1dbe11a6-adc1-4fb3-b600-45659c3764ac"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  # Essentially this will create a new model that stops just short of the\n",
        "  # final classification layer and instead just outputs powerful feature vector.\n",
        "    def __init__(self, original_model):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        # take all the layers of the original model EXCEPT for the last one (fc layer)\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input image through all the convolutional layers.\n",
        "        # This output from ResNet feature layer ends up being a 4D tensor. Which we need to flatten.\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1) # we flatten this to the shape [1, 512] to get our final embedding\n",
        "        return x\n",
        "\n",
        "# Create instance of our feature extractor using the classifier we just loaded.\n",
        "if 'model_to_load' in locals() and model_to_load is not None:\n",
        "    embedding_model = FeatureExtractor(model_to_load)\n",
        "    embedding_model = embedding_model.to(device)\n",
        "    embedding_model.eval() # Set to evaluation mode for inference\n",
        "    print(\"Feature extractor embedding created.\")\n",
        "    print(f\"Output embedding dimension: {num_ftrs}\") # Should be 512 for ResNet18 (check)\n",
        "else:\n",
        "    print(\"feature embedding not created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj5IqJ0NYZym"
      },
      "outputs": [],
      "source": [
        "# To get meaningful results, any new image we show the model must be processed\n",
        "# in the exact same way as the images it saw during training/validation.\n",
        "# This includes resizing, cropping, converting to a tensor, and normalizing.\n",
        "\n",
        "input_size = 224 # ResNet input size\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize(input_size + 32), # Resize slightly larger\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet stats\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-rEokATYcy7",
        "outputId": "f03232b4-5351-4738-c2e0-3e30359b5b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of extracted embedding: (1, 512)\n",
            "Embedding vector: [[1.58542216e+00 1.46742627e-01 9.99294281e-01 4.72742766e-02\n",
            "  1.87943816e+00 8.88246179e-01 1.20832741e-01 6.95589185e-01\n",
            "  9.97585654e-02 6.87132239e-01 6.70889020e-01 8.58204722e-01\n",
            "  0.00000000e+00 2.55493373e-01 5.16166016e-02 9.43767607e-01\n",
            "  8.78534615e-01 5.90496659e-02 1.61917007e+00 1.21391334e-01\n",
            "  5.18330932e-02 2.24496722e+00 9.87011492e-01 8.59058321e-01\n",
            "  1.92775333e+00 8.01303238e-03 6.00792095e-03 1.18445909e+00\n",
            "  2.99010277e-01 3.44768018e-01 9.25906301e-01 9.64122355e-01\n",
            "  5.31638443e-01 2.36872345e-01 1.73967153e-01 5.54729402e-01\n",
            "  1.28029609e+00 7.03313425e-02 1.37593424e+00 5.11817098e-01\n",
            "  3.98509502e-01 9.71403599e-01 2.16634616e-01 1.04438707e-01\n",
            "  1.10476899e+00 2.45347977e+00 7.73929119e-01 8.33565712e-01\n",
            "  4.40420419e-01 2.24054193e+00 1.29326925e-01 6.81358576e-01\n",
            "  0.00000000e+00 7.67900348e-01 1.97619915e+00 5.90615720e-02\n",
            "  2.17641830e+00 4.37796414e-01 3.67841184e-01 4.10303265e-01\n",
            "  9.95721743e-02 3.33982438e-01 4.39094193e-02 1.33351445e+00\n",
            "  3.20764184e-01 7.33163059e-02 1.35437691e+00 7.92436078e-02\n",
            "  6.16748869e-01 1.35040069e+00 3.88440371e-01 1.24111044e+00\n",
            "  1.87967569e-01 6.66001797e-01 1.03686303e-01 1.70211643e-01\n",
            "  1.39279497e+00 1.25412309e+00 2.31521010e+00 2.83012360e-01\n",
            "  1.02196723e-01 8.03063631e-01 2.67145514e-01 1.80315852e+00\n",
            "  8.92062068e-01 5.80194950e-01 7.48855472e-02 5.40975153e-01\n",
            "  3.20796575e-03 1.52399629e-01 1.78098872e-01 6.20406687e-01\n",
            "  3.56758833e-01 1.80448145e-02 2.17071436e-02 2.81655759e-01\n",
            "  1.19794881e+00 3.45601141e-01 1.66586399e+00 9.49875057e-01\n",
            "  1.50286770e+00 5.12212396e-01 5.34910560e-01 8.60926330e-01\n",
            "  1.77349135e-01 1.42656898e+00 4.83166516e-01 5.84473908e-01\n",
            "  1.53657436e+00 2.38172010e-01 3.58129619e-03 1.65060031e+00\n",
            "  6.95676506e-01 6.70748591e-01 7.25311458e-01 4.93339837e-01\n",
            "  2.90187472e-03 2.16455702e-02 8.10326099e-01 2.20737666e-01\n",
            "  5.70766270e-01 3.49751681e-01 6.65746868e-01 1.05297494e+00\n",
            "  1.00392592e+00 2.04300356e+00 1.60504603e+00 8.31696749e-01\n",
            "  2.29377165e-01 3.10421515e+00 1.52659055e-03 2.44157135e-01\n",
            "  4.08870161e-01 1.02892530e+00 1.43005118e-01 8.80248472e-02\n",
            "  3.19941819e-01 1.09473610e+00 2.83667743e-01 2.88355023e-01\n",
            "  4.49096143e-01 6.31303200e-03 4.70625639e-01 1.07725799e+00\n",
            "  2.64612287e-01 4.21554297e-02 1.22036010e-01 1.24310830e-03\n",
            "  9.69073474e-01 3.34322631e-01 1.01597297e+00 7.05190837e-01\n",
            "  3.14203024e-01 2.25826097e+00 2.72043884e-01 1.28231514e+00\n",
            "  6.98066736e-03 5.09605568e-04 8.33166599e-01 7.03175008e-01\n",
            "  2.01798034e+00 1.20141363e+00 2.88293743e+00 2.19824696e+00\n",
            "  1.11125946e+00 1.38077128e+00 2.52232462e-01 3.48151177e-01\n",
            "  2.85934359e-01 2.25963011e-01 1.05120182e+00 7.83071578e-01\n",
            "  2.78818421e-02 2.61477888e-01 1.03407064e-02 6.96399391e-01\n",
            "  1.72969013e-01 1.65622756e-01 1.09224844e+00 2.79343396e-01\n",
            "  6.60472214e-02 1.25405228e+00 8.89231324e-01 0.00000000e+00\n",
            "  3.60234082e-01 5.15176728e-03 1.17417359e+00 8.65995526e-01\n",
            "  9.10118699e-01 4.87794846e-01 2.57659256e-01 2.82163352e-01\n",
            "  3.71975675e-02 2.01250404e-01 6.14006341e-01 1.70600340e-01\n",
            "  1.45677590e+00 1.64078072e-01 1.52790082e+00 3.02939653e-01\n",
            "  4.05278653e-01 1.33441508e+00 9.36279833e-01 9.56577361e-01\n",
            "  1.53770745e+00 6.87899351e-01 3.43381912e-01 3.51782925e-02\n",
            "  2.11655721e-01 4.19160992e-01 7.02965200e-01 1.57559264e+00\n",
            "  1.52633655e+00 6.73086345e-01 1.98215812e-01 2.65810639e-01\n",
            "  2.53856921e+00 1.91300064e-02 3.48445356e-01 1.07274485e+00\n",
            "  8.98607731e-01 1.49296200e+00 1.29539478e+00 3.29981446e-01\n",
            "  3.99150252e-02 3.33594799e-01 8.01079571e-01 9.24763754e-02\n",
            "  5.04817247e-01 1.74597418e+00 5.78112125e-01 1.84764504e-01\n",
            "  1.44408894e+00 2.71319240e-01 1.03658772e+00 1.48178804e+00\n",
            "  2.74900999e-02 4.56029862e-01 8.73817682e-01 1.00699675e+00\n",
            "  1.25321221e+00 5.77461980e-02 8.91271159e-02 3.16964924e-01\n",
            "  0.00000000e+00 5.31221390e-01 1.37824965e+00 1.34548140e+00\n",
            "  4.52967882e-01 2.31789023e-01 1.24437809e+00 9.99807835e-01\n",
            "  1.20923150e+00 1.65245682e-01 2.84316950e-02 2.79449081e+00\n",
            "  1.19692409e+00 4.62566167e-01 2.02074695e+00 4.91243213e-01\n",
            "  2.43281852e-02 4.88668727e-03 1.82801075e-02 1.11880088e+00\n",
            "  2.13440824e+00 6.89277470e-01 6.00470901e-01 1.42109644e+00\n",
            "  1.54595733e-01 1.26681423e+00 6.11785173e-01 1.46299684e+00\n",
            "  1.80008352e+00 1.16756845e+00 1.09519458e+00 2.24567428e-01\n",
            "  1.66594613e+00 8.07189107e-01 7.02043593e-01 2.56067701e-02\n",
            "  2.85908914e+00 7.98796415e-02 3.75019699e-01 0.00000000e+00\n",
            "  1.19003141e+00 1.40139687e+00 1.91900754e+00 3.99795920e-01\n",
            "  0.00000000e+00 8.47579896e-01 4.13147241e-01 1.84982806e-01\n",
            "  3.59392285e-01 1.25700223e+00 1.52222425e-01 4.55024868e-01\n",
            "  1.12069033e-01 5.21645665e-01 1.50316045e-01 2.86486506e-01\n",
            "  1.51434898e+00 1.36510044e-01 5.27385414e-01 2.63604617e+00\n",
            "  5.43556988e-01 4.73491522e-03 3.57035577e-01 2.71313357e+00\n",
            "  1.38218975e+00 1.56399354e-01 1.64211720e-01 5.58675826e-02\n",
            "  3.35771069e-02 1.06451139e-01 5.59493303e-01 1.83805823e-01\n",
            "  5.20957351e-01 1.20273912e+00 5.53473473e-01 1.07701018e-01\n",
            "  8.73494372e-02 2.30262160e-01 1.15248427e-01 1.83483076e+00\n",
            "  2.14702606e-01 1.01466250e+00 1.03816688e+00 1.01947570e+00\n",
            "  3.55909258e-01 7.93054461e-01 8.90595555e-01 1.40797842e+00\n",
            "  2.50565521e-02 1.22861445e+00 1.12869829e-01 1.14883101e+00\n",
            "  6.88971043e-01 1.39561105e+00 1.21768802e-01 1.27278650e+00\n",
            "  9.02018428e-01 4.51550126e-01 2.99955547e-01 1.80109560e+00\n",
            "  7.88372993e-01 3.77043009e-01 3.56449448e-02 1.20240521e+00\n",
            "  1.19399297e+00 1.50316641e-01 9.00723517e-01 7.90869415e-01\n",
            "  5.44876873e-01 1.07232583e+00 3.23663801e-02 7.91439116e-01\n",
            "  1.03312910e+00 1.11812401e+00 1.18508565e+00 2.63806313e-01\n",
            "  1.32829234e-01 1.29060936e+00 1.36946428e+00 1.71509355e-01\n",
            "  8.49470496e-01 1.97821155e-01 7.29609728e-01 1.35390535e-01\n",
            "  1.71261594e-01 3.45369190e-01 6.82033718e-01 2.86535680e-01\n",
            "  7.78957546e-01 1.49524570e+00 1.46741718e-02 6.59027696e-01\n",
            "  1.59141135e+00 3.42376709e-01 1.72297075e-01 7.10658610e-01\n",
            "  1.56723075e-02 1.35378718e+00 1.42085004e+00 7.14068189e-02\n",
            "  2.10841489e+00 3.35955650e-01 3.95010650e-01 5.02307653e-01\n",
            "  1.47365618e+00 9.59048688e-01 1.62803173e+00 2.11717367e+00\n",
            "  5.18576264e-01 2.81272292e-01 1.99057364e+00 9.37064826e-01\n",
            "  9.32897776e-02 9.66317832e-01 5.17371178e-01 2.50942763e-02\n",
            "  3.72158527e-01 6.98660016e-01 4.08766031e-01 3.33445311e-01\n",
            "  2.18025073e-01 1.54166654e-01 1.50708413e+00 2.85462849e-02\n",
            "  5.04292905e-01 1.61184478e+00 9.12420988e-01 6.42422557e-01\n",
            "  1.76621512e-01 7.41469920e-01 1.76110291e+00 9.08325791e-01\n",
            "  4.50626994e-03 1.00611746e-01 1.26196539e+00 7.75007978e-02\n",
            "  9.80552614e-01 8.57059181e-01 9.40858573e-03 1.50028765e-01\n",
            "  1.84883624e-02 9.77897644e-01 0.00000000e+00 1.18218422e+00\n",
            "  5.15407696e-03 1.49066019e+00 8.81700099e-01 5.84730566e-01\n",
            "  2.88370132e-01 1.19230688e+00 2.71012187e-01 4.48911905e-01\n",
            "  3.97575758e-02 0.00000000e+00 3.60687256e-01 6.09528542e-01\n",
            "  6.11211419e-01 1.24464321e+00 9.38536972e-02 1.18158734e+00\n",
            "  4.04649787e-02 1.26997912e+00 3.58062029e-01 7.00614631e-01\n",
            "  6.37730837e-01 3.59768905e-02 1.15653527e+00 2.18474016e-01\n",
            "  1.48219872e+00 8.93582404e-02 1.08904374e+00 1.92242131e-01\n",
            "  2.30711386e-01 5.05146801e-01 2.08019316e-02 1.55923679e-01\n",
            "  1.45143986e-01 4.28070687e-03 8.76297772e-01 1.56063342e+00\n",
            "  1.44549295e-01 1.86937198e-01 2.93510747e+00 3.94606739e-01\n",
            "  2.15454642e-02 3.82122174e-02 4.11234796e-01 1.60633564e+00\n",
            "  7.65092909e-01 5.69921970e-01 5.27616084e-01 9.76472020e-01\n",
            "  5.95749617e-01 3.65611874e-02 2.78811860e+00 1.43531859e+00\n",
            "  1.47971034e-01 1.00522983e+00 3.28045279e-01 1.15723252e+00\n",
            "  9.31560695e-01 2.36171097e-01 1.04364014e+00 7.85359293e-02\n",
            "  7.81543851e-02 1.14453517e-01 2.13734531e+00 5.98022155e-03\n",
            "  2.07212698e-02 2.94677973e-01 3.97866994e-01 8.65231037e-01\n",
            "  1.70117557e+00 3.27273667e-01 6.33144677e-01 1.55392742e+00\n",
            "  5.56904078e-01 7.46657372e-01 1.07148194e+00 3.32556248e-01\n",
            "  1.09639382e+00 1.89526707e-01 3.48455966e-01 2.67009467e-01\n",
            "  1.03911115e-02 9.94669497e-02 2.74463922e-01 8.57418478e-02]]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# This function takes a single image path, processes it, and returns its embedding vector.\n",
        "def get_embedding(image_path, model, transform, device):\n",
        "    try:\n",
        "        # open using the Pillow library to ensure it's in RGB format.\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Image not found at {image_path}\")\n",
        "        return None\n",
        "\n",
        "    img_transformed = transform(img)\n",
        "    # PyTorch models expect a batch of images, even if we're only processing one\n",
        "    # .unsqueeze(0) adds a new dimension, changing the shape from [3, 224, 224] to [1, 3, 224, 224].\n",
        "    img_batch = img_transformed.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad(): # Important: no gradients needed for inference (only training)\n",
        "        embedding = model(img_batch)\n",
        "\n",
        "    return embedding.cpu().numpy() # Return embedding as a NumPy array\n",
        "\n",
        "# Example test\n",
        "example_image_path = '/content/drive/MyDrive/dataset/validation/person_pryce/pryce_close_up_frame_00003.jpg' # CHANGE THIS for each different image test\n",
        "\n",
        "if 'embedding_model' in locals() and os.path.exists(example_image_path):\n",
        "  example_embedding = get_embedding(example_image_path, embedding_model, inference_transform, device)\n",
        "  if example_embedding is not None:\n",
        "    print(f\"Shape of extracted embedding: {example_embedding.shape}\") # Should be (1, 512)\n",
        "    print(f\"Embedding vector: {example_embedding}\")\n",
        "else:\n",
        "  print(\"Example Embedding did not work.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w67V7qIsDDqh"
      },
      "source": [
        "This is where we grab the invididual embeddings of all our images and average them. I've done this for everyone so far and I can send the .npy download soon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "725ghjmKCEtu",
        "outputId": "191e8647-0008-453f-bbeb-f12b1911041d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing images for: person_ben\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7093_frame_00370.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7093_frame_00327.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7093_frame_00319.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7093_frame_00036.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7093_frame_00138.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7092_frame_00700.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7092_frame_00597.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7092_frame_00539.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7092_frame_00479.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n",
            "  Getting embedding for: /content/drive/MyDrive/dataset/validation/person_ben/ben_IMG_7092_frame_00500.jpg\n",
            "    Successfully retrieved embedding with shape: (512,)\n"
          ]
        }
      ],
      "source": [
        "# The goal here is to create a single embedding for a person.\n",
        "# A single photo might have odd lighting or a strange expression. By averaging the\n",
        "# embeddings from several good, representative photos, we create a more robust and\n",
        "# stable representation of that person.\n",
        "\n",
        "import numpy as np\n",
        "validation_folder_base_path = '/content/drive/MyDrive/dataset/validation/'\n",
        "\n",
        "# Which person we want to create a prototype for\n",
        "person_id = 'person_ben'\n",
        "person_specific_validation_folder = os.path.join(validation_folder_base_path, person_id)\n",
        "\n",
        "# Manually choose 10 of the best images for this person from the validation folder\n",
        "# Replace these with the actual filenames of your chosen images\n",
        "best_image_filenames_for_person = [\n",
        "    'ben_IMG_7093_frame_00370.jpg',\n",
        "    'ben_IMG_7093_frame_00327.jpg',\n",
        "    'ben_IMG_7093_frame_00319.jpg',\n",
        "    'ben_IMG_7093_frame_00036.jpg',\n",
        "    'ben_IMG_7093_frame_00138.jpg',\n",
        "    'ben_IMG_7092_frame_00700.jpg',\n",
        "    'ben_IMG_7092_frame_00597.jpg',\n",
        "    'ben_IMG_7092_frame_00539.jpg',\n",
        "    'ben_IMG_7092_frame_00479.jpg',\n",
        "    'ben_IMG_7092_frame_00500.jpg',\n",
        "]\n",
        "\n",
        "#list holds the embedding vector for each of the selected images.\n",
        "person_embeddings_list = []\n",
        "\n",
        "print(f\"Processing images for: {person_id}\")\n",
        "for image_filename in best_image_filenames_for_person:\n",
        "    image_path = os.path.join(person_specific_validation_folder, image_filename)\n",
        "\n",
        "    print(f\"  Getting embedding for: {image_path}\")\n",
        "    # Use the get_embedding function we defined earlier\n",
        "    embedding_vector_batch = get_embedding(image_path, embedding_model, inference_transform, device)\n",
        "\n",
        "    if embedding_vector_batch is not None:\n",
        "        # The get_embedding function returns a shape like (1, 512)\n",
        "        # We need to get the actual (512,) array out of the batch\n",
        "        embedding_vector_single = embedding_vector_batch[0]\n",
        "        person_embeddings_list.append(embedding_vector_single)\n",
        "        print(f\"    Successfully retrieved embedding with shape: {embedding_vector_single.shape}\")\n",
        "    else:\n",
        "        print(f\"    Warning: Could not retrieve embedding for {image_path}\")\n",
        "\n",
        "# Make sure to get these for each person so we can make averages of the embeddings to compare!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA69yNo_Cd0M",
        "outputId": "df332528-4c23-4839-e536-f52a77dd47b9"
      },
      "outputs": [],
      "source": [
        "if person_embeddings_list:\n",
        "    if len(person_embeddings_list) == len(best_image_filenames_for_person):\n",
        "        print(f\"\\ncollected {len(person_embeddings_list)} embeddings.\")\n",
        "    else:\n",
        "        print(f\"\\ncollected {len(person_embeddings_list)} embeddings out of {len(best_image_filenames_for_person)}. error\")\n",
        "\n",
        "    # np.mean with axis=0 calculates the average value\n",
        "    # for each of the 512 dimensions across all the gathered embedding vectors.\n",
        "    prototype_embedding = np.mean(person_embeddings_list, axis=0)\n",
        "\n",
        "    print(f\"\\nAveraged embedding for {person_id}:\")\n",
        "    print(f\"  Shape: {prototype_embedding.shape}\") # Should be (512,)\n",
        "    print(f\"  Averaged embedding: {prototype_embedding}\")\n",
        "\n",
        "    # save this averaged embedding to a .npy file\n",
        "    # Our real-time application will load these .npy files to create its gallery of known people.\n",
        "    prototype_save_path = f'/content/drive/MyDrive/{person_id}_prototype_embedding.npy'\n",
        "    np.save(prototype_save_path, prototype_embedding)\n",
        "    print(f\" embedding saved to: {prototype_save_path}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\nNo embeddings for {person_id}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
